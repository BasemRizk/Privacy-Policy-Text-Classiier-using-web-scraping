{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9834803",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/basemrizk/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/basemrizk/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/basemrizk/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/basemrizk/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import libraries\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "from itertools import zip_longest\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import string\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "#apply linear kernel svm model with 5 cv folds\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "#Accuracy report for category\n",
    "from sklearn import metrics\n",
    "#import nltk to tokenize the predicted data to be shown\n",
    "import nltk.data\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7fc4252",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove urls and clean html if it appeared in the paragraphs or list\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "    return url.sub(r\"\", text)\n",
    "CLEANR = re.compile('<.*?>') \n",
    "\n",
    "def remove_tags(text):\n",
    "    tags_list = ['<p>' ,'</p>' , '<p*>',\n",
    "             '<ul>','</ul>',\n",
    "             '<li>','</li>',\n",
    "             '<br>',\n",
    "             '<strong>','</strong>',\n",
    "             '<span*>','</span>',\n",
    "             '<a href*>','</a>',\n",
    "             '<em>','</em>','â†’']\n",
    "    for tag in tags_list:\n",
    "        text = text.replace(tag, '')\n",
    "    return text\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "#remove punctation\n",
    "def remove_punct(text):\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "    return text.translate(translator)\n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "def cleanhtml(raw_html):\n",
    "  cleantext = re.sub(CLEANR, '', raw_html)\n",
    "  return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "be3ab623",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read category file (load file(11.csv))\n",
    "df=pd.read_csv('11.csv')\n",
    "#read group file (load file (11_G.csv))\n",
    "dfgroup=pd.read_csv(\"11_G.csv\")\n",
    "#save the groups as string in new column grp\n",
    "dfgroup['grp']=dfgroup['group']\n",
    "#save the groups names as int in column group\n",
    "dfgroup.group=dfgroup.group.replace({'A':0,'B':1,'C':2})\n",
    "# save the group values as the labels for classification process\n",
    "ygroup=dfgroup['group']\n",
    "#list the group names in an array\n",
    "group_names=np.array(['A','B','C'])\n",
    "#if you find any word out of vocabulary give it the value OOV\n",
    "oov_tok=\"<OOV>\"\n",
    "#save the category as string in new column cat\n",
    "df['cat']=df['category']\n",
    "#save the category names as int in column category\n",
    "df.category=df.category.replace({\"First Party Collection/Use\":0,\n",
    "\"Third Party Sharing/Collection\":1,\"User Choice/Control\":2,\"User Access, Edit and Deletion\":3,\n",
    "\"Data Retention\":4,\"Data Security\":5,\"Policy Change\":6,\"Do Not Track\":7 ,\"International and Specific Audiences\":8,\n",
    "\"Other\":9})\n",
    "#list the category names in an array\n",
    "target_names=['First Party Collection/Use','Third Party Sharing/Collection','User Choice/Control','User Access, Edit and Deletion','Data Retention','Data Security','Policy Change','Do Not Track','International and Specific Audiences','Other']\n",
    "#list the group names in an array\n",
    "category_name=np.array(target_names)\n",
    "\n",
    "#removing url for input of the model\n",
    "df[\"text\"] = df.text.map(remove_URL)\n",
    "df[\"text\"] = df.text.map(cleanhtml)\n",
    "\n",
    "#removing tags for input of the model\n",
    "df['text'] = df['text'].apply(remove_tags)\n",
    "\n",
    "#removing stopwords for the model input\n",
    "df[\"text\"] = df.text.map(remove_stopwords)\n",
    "#removing punctation for the model input\n",
    "df[\"text\"] = df.text.map(remove_punct)\n",
    "#lemmatizing the input\n",
    "df['text'] = df.text.map(lemmatize_text)\n",
    "#rejoin the words to sentences\n",
    "for i in range(len(df)):\n",
    "  df.text[i] = ' '.join(df.text[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7eb8f6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "          First Party Collection/Use       0.78      0.83      0.80       290\n",
      "      Third Party Sharing/Collection       0.81      0.77      0.79       209\n",
      "                 User Choice/Control       0.73      0.57      0.64        77\n",
      "      User Access, Edit and Deletion       0.68      0.70      0.69        27\n",
      "                      Data Retention       1.00      0.29      0.44         7\n",
      "                       Data Security       0.89      0.82      0.85        39\n",
      "                       Policy Change       0.94      0.91      0.92        32\n",
      "                        Do Not Track       1.00      1.00      1.00         4\n",
      "International and Specific Audiences       0.86      0.89      0.87        62\n",
      "                               Other       0.74      0.79      0.76       201\n",
      "\n",
      "                            accuracy                           0.79       948\n",
      "                           macro avg       0.84      0.76      0.78       948\n",
      "                        weighted avg       0.79      0.79      0.78       948\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#split the data to train and test data \n",
    "\n",
    "x=df.text\n",
    "y=df.category\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.25,random_state=4,shuffle=True)\n",
    "#transform the input text into count vectorizer\n",
    "count_vect=CountVectorizer()\n",
    "x_train_count=count_vect.fit_transform(x_train)\n",
    "#apply tfid transform on the text\n",
    "\n",
    "tfid_transformer=TfidfTransformer()\n",
    "x_train_tfidf=tfid_transformer.fit_transform(x_train_count)\n",
    "\n",
    "text_clf=Pipeline([('tfidf',TfidfVectorizer()),\n",
    "                   ('clf',SVC(kernel='linear',C=1,gamma='auto',probability=False,random_state=0)),])\n",
    "text_clf.fit(x_train,y_train)\n",
    "#make predictions on the test data to check accuracy\n",
    "SVMpredictions=text_clf.predict(x_test)\n",
    "print (metrics.classification_report(y_test,SVMpredictions,target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcb0bea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['privacy_clf.pkl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model as a file\n",
    "joblib.dump(text_clf, 'privacy_clf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7808be13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
